{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook: analyze_text.ipynb\n",
    "\n",
    "This notebook extracts features from user tweets and standardizes the data at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "df = pd.read_json('../datasets/MIB/mib_processed.json')\n",
    "classifier_columns = ['identification', 'geo_enabled','default_profile','default_profile_image','followers_count','friends_count','favourites_count','listed_count','retweet_post_percent','reply_post_percent','avg_hashtags','avg_urls','avg_mentions','avg_retweets_cnt','avg_reply_cnt', 'unique_words_per_word', 'avg_neg_sentiment', 'avg_neu_sentiment', 'avg_pos_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['num_unique_words', 'total_words', 'unique_words_per_word',\n",
    "               'avg_neg_sentiment', 'avg_neu_sentiment', 'avg_pos_sentiment']\n",
    "\n",
    "# add new columns to the dataframe if not already present\n",
    "def add_columns(df, columns):\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "# standardize all numeric values in the dataframe\n",
    "def standardize(df, final_columns):\n",
    "    df = df[final_columns]\n",
    "    \n",
    "    # find all columns that are not boolean values\n",
    "    numeric_cols = [col for col in df if (col != 'recent_tweet_text' and len(df[col].dropna().unique()) > 2)]\n",
    "    temp_df = df[numeric_cols]\n",
    "    df[numeric_cols] = (temp_df - temp_df.mean())/temp_df.std()\n",
    "    return df.fillna(0)\n",
    "\n",
    "add_columns(df, new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "# some tweets are in italian\n",
    "sw.update(set(stopwords.words('italian')))\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    tweets = row['recent_tweet_text']\n",
    "    total_words = 0\n",
    "    words = set()\n",
    "    neg_sentiment = neu_sentiment = pos_sentiment = 0\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        if tweet:\n",
    "            # extract average sentiment across user's tweets\n",
    "            polarity_scores = sentiment_analyzer.polarity_scores(tweet)\n",
    "            neg_sentiment += polarity_scores['neg']\n",
    "            neu_sentiment += polarity_scores['neu']\n",
    "            pos_sentiment += polarity_scores['pos']\n",
    "            \n",
    "            # get a set of all unique words and count of total words across user's tweets\n",
    "            tkn = tokenizer.tokenize(tweet)\n",
    "            total_words += len(tkn)\n",
    "            words.update({word for word in tkn if word not in sw})\n",
    "            tokens.append(tkn)\n",
    "    \n",
    "    df.at[index, 'num_unique_words'] = len(words)\n",
    "    df.at[index, 'total_words'] = total_words\n",
    "    df.at[index, 'avg_neg_sentiment'] = neg_sentiment/len(tweets)\n",
    "    df.at[index, 'avg_neu_sentiment'] = neu_sentiment/len(tweets)\n",
    "    df.at[index, 'avg_pos_sentiment'] = pos_sentiment/len(tweets)\n",
    "\n",
    "df['unique_words_per_word'] = df['num_unique_words']/df['total_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_df = standardize(df, classifier_columns)\n",
    "standardized_df.to_csv('../datasets/MIB/mib_processed_text_standardized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
